{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Text Prompt Design: Challenge Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "This challenge lab is designed to test your knowledge of calling Gemini and utilizing a few fundamental text prompt design techniques.\n",
    "\n",
    "Two featured guides on prompting from the Google Cloud documentation are:\n",
    "\n",
    "1. [Overview of prompting strategies](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) from the Generative AI on Vertex AI documentation.\n",
    "\n",
    "2. [Prompt design strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies) from the Gemini API documentation.\n",
    "\n",
    "Both contain good tips. You are encouraged to **bookmark them**.\n",
    "\n",
    "## Objective\n",
    "You will demonstrate your ability to:\n",
    "\n",
    "- Initialize Vertex AI in your environment\n",
    "- Load a generative model\n",
    "- Guide model output with a persona\n",
    "- Extract information to a schema\n",
    "- Stay on topic with fallback responses\n",
    "- Use examples to influence the model's response\n",
    "\n",
    "Some of the following Python notebook cells have missing or incomplete code sections and tasks that need to be completed, indicated by the code comments starting with `# TODO`. Your challenge is to complete each cell, run it to test for correctness, and then move on. When all the cells are working, you have completed the challenge.\n",
    "\n",
    "**Note:** If you need help, [this notebook demonstrates getting started using Gemini in Python](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_python.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "## Task 1. Install, import & initialize the Vertex AI SDK and a generative model\n",
    "\n",
    "1. Install the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the following pip command\n",
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "2. Restart your notebook kernel.\n",
    "3. Import the following:\n",
    "- the Vertex AI SDK\n",
    "- the class to instantiate a generative model from the Vertex AI generative models module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "# The required imports are already done in CELL INDEX: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kpdnPWaTK27C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Complete the imports\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "4. Initialize Vertex AI with your project ID and a location (you can use `us-central1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"qwiklabs-gcp-01-e30e47f7c3c5\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LCaCx6PLSBW6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Insert the required steps here\n",
    "vertexai.init(project=\"qwiklabs-gcp-01-e30e47f7c3c5\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "# aiplatform.init(project=\"qwiklabs-gcp-01-e30e47f7c3c5\", location=\"us-central1\")\n",
    "client = aiplatform.init(project=\"qwiklabs-gcp-01-e30e47f7c3c5\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8DUcgKJK27C"
   },
   "source": [
    "5. Instantiate a generative model and save it to the `generative_model` variable. For this notebook, use `gemini-pro` as your model version. When instantiating the model, pass a `generation_config` parameter with the temperature set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2998506fe6d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Insantiate a \"gemini-pro\" model with a configured temperature of 0.\n",
    "generative_model = GenerativeModel(\"gemini-pro\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3YN2rf1K27C"
   },
   "source": [
    "6. Complete the TODO's in this function, which you will use for the rest of the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joXl2V2JK27D"
   },
   "outputs": [],
   "source": [
    "def print_response(prompt):\n",
    "\n",
    "    # TODO: Complete this line to generate a response to the prompt:\n",
    "    response = generative_model.generate(prompt)\n",
    "\n",
    "    # TODO: Complete this line to print only the text of the model's response,\n",
    "    # not the additional response metadata.\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "## Task 2. Personas\n",
    "\n",
    "1. Run the following cell to see the default response to this prompt.\n",
    "\n",
    "2. Then tweak the prompt by asking the model to take on the persona of an **energetic, inspiring personal trainer** who can get users **excited to work out their leg muscles**. Note the difference in vocabulary and tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEo0QhauK27D"
   },
   "outputs": [],
   "source": [
    "# TODO: Improve the personality of the response by assigning the suggested persona. tweak the prompt by asking the model to take on the persona of an **energetic, inspiring personal trainer** who can get users **excited to work out their leg muscles**. Note the difference in vocabulary and tone.\n",
    "prompt = \"What are some good leg exercises?\"\n",
    "# Modify the prompt to include the persona description\n",
    "prompt = \"As an energetic, inspiring personal trainer, what are some good leg exercises to get users excited to work out their leg muscles?\"\n",
    "\n",
    "# Call the function to print the response\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueKZMheeK27D"
   },
   "source": [
    "## Task 3. Be specific + constrain the output format\n",
    "\n",
    "1. Have the model convert the following text of cooking ingredients to a YAML format. Each ingredient should be listed as a dictionary with keys for **ingredient** and **quantity** populated with the correct value given the ingredients in the following recipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyJASDrsK27D"
   },
   "outputs": [],
   "source": [
    "# TODO: Write instructions to complete the task.\n",
    "instructions = \"Convert the following list of cooking ingredients into a YAML format. Each ingredient should be listed as a dictionary with keys for 'ingredient' and 'quantity'.\"\n",
    "\n",
    "ingredients = \"\"\"\n",
    "    Ingredients:\n",
    "    * 9 egg whites\n",
    "    * 3/8 tsp Cream of Tartar\n",
    "    * 1 1/2 tbs Vinegar\n",
    "    * 1 1/2 tsp Vanilla\n",
    "    * 3 cups Sugar\n",
    "    * 1 quarts Heavy whipping cream\n",
    "    * 3 boxes Strawberries\n",
    "    \"\"\"\n",
    "\n",
    "prompt = instructions + \"\\n\\n\" + ingredients\n",
    "\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xRk8HTLK27D"
   },
   "source": [
    "## Task 4. Use a fallback response\n",
    "\n",
    "1. Adjust the prompt below to specify that the model should only answer questions related to historical landmarks. If a user askes about something else, the model should respond with the message: `Sorry, I only answer questions about historical landmarks!`\n",
    "\n",
    "2. Adjust your instructions until the model declines to answer the `user_query` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6InaQ3OpK27D"
   },
   "outputs": [],
   "source": [
    "# TODO: Add instructions to prompt a fallback response for off-topic queries.\n",
    "\n",
    "instructions = \"You are a history tour guide. Answer the user's question: {user_query}\"\n",
    "\n",
    "user_query = \"How can I attract butterflies to my garden?\"\n",
    "\n",
    "print_response(instructions.format(user_query=user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add instructions to prompt a fallback response for off-topic queries.\n",
    "instructions = \"\"\"\n",
    "You are a history tour guide. Answer only questions related to historical landmarks. \n",
    "If the question is not about historical landmarks, respond with: \n",
    "'Sorry, I only answer questions about historical landmarks!'\n",
    "\"\"\"\n",
    "\n",
    "user_query = \"How can I attract butterflies to my garden?\"\n",
    "\n",
    "prompt = instructions + \"\\n\\n\" + \"User's question: \" + user_query\n",
    "\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFolKJjNK27D"
   },
   "source": [
    "## Task 5. Make results more specific with examples\n",
    "\n",
    "1. Run the code cell below to see the model's response as-is.\n",
    "\n",
    "2. Imagining you work for a bicycle tour company, modify each of the example outputs below to include a bicycle.\n",
    "\n",
    "3. Re-run the code cell to make sure the model generates a bicycle-themed response. Leave the instructions alone and tweak your examples until you get such a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsjp39ZWK27D"
   },
   "outputs": [],
   "source": [
    "# TODO: Modify the examples below to guide the model to always generate\n",
    "# recommendations involving bicycles.\n",
    "\n",
    "prompt = \"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    Give a tourist recommendation for the input city.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <EXAMPLES>\n",
    "    Input: Paris\n",
    "    Output: Take a taxi to the Louvre and then to Montmartre.\n",
    "\n",
    "    Input: Washington D.C.\n",
    "    Output: Drive your rental car to the Lincoln Memorial.\n",
    "\n",
    "    Input: New York City\n",
    "    Output: Walk along the river.\n",
    "    </EXAMPLES>\n",
    "\n",
    "    <INPUT CITY>\n",
    "    Bangalore\n",
    "    </INPUT CITY>\"\"\"\n",
    "\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the examples below to guide the model to always generate\n",
    "# recommendations involving bicycles.\n",
    "\n",
    "prompt = \"\"\"\n",
    "    <INSTRUCTIONS>\n",
    "    Give a tourist recommendation for the input city.\n",
    "    </INSTRUCTIONS>\n",
    "\n",
    "    <EXAMPLES>\n",
    "    Input: Paris\n",
    "    Output: Rent a bicycle and ride to the Louvre, then cycle to Montmartre.\n",
    "\n",
    "    Input: Washington D.C.\n",
    "    Output: Rent a bicycle and ride to the Lincoln Memorial, then cycle around the National Mall.\n",
    "\n",
    "    Input: New York City\n",
    "    Output: Rent a bicycle and ride along the river, then cycle through Central Park.\n",
    "    </EXAMPLES>\n",
    "\n",
    "    <INPUT CITY>\n",
    "    Bangalore\n",
    "    </INPUT CITY>\"\"\"\n",
    "\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvbIXnwqK27D"
   },
   "source": [
    "## Congratulations!\n",
    "\n",
    "If you have completed the steps above, you have demonstrated your ability to use several prompt engineering techniques."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_prompt_design_challenge_lab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
